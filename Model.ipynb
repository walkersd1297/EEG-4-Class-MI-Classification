{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, AveragePooling2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, SeparableConv2D, DepthwiseConv2D\n",
    "from tensorflow.keras.layers import BatchNormalization, LayerNormalization, Flatten \n",
    "from tensorflow.keras.layers import Add, Concatenate, Lambda, Input, Permute\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from attention_models import attention_block\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path, subject, training, all_trials = False):\n",
    "    \"\"\" Loading and Dividing of the data set based on the subject-specific \n",
    "    (subject-dependent) approach.\n",
    "    In this approach, we used the same training and testing dataas the original\n",
    "    competition, i.e., 288 x 9 trials in session 1 for training, \n",
    "    and 288 x 9 trials in session 2 for testing.  \n",
    "   \n",
    "        Parameters\n",
    "        ----------\n",
    "        data_path: string\n",
    "            dataset path\n",
    "            # Dataset BCI Competition IV-2a is available on \n",
    "            # http://bnci-horizon-2020.eu/database/data-sets\n",
    "        subject: int\n",
    "            number of subject in [1, .. ,9]\n",
    "        training: bool\n",
    "            if True, load training data\n",
    "            if False, load testing data\n",
    "        all_trials: bool\n",
    "            if True, load all trials\n",
    "            if False, ignore trials with artifacts \n",
    "    \"\"\"\n",
    "    # Define MI-trials parameters\n",
    "    n_channels = 22\n",
    "    n_tests = 6*48 \t\n",
    "    window_Length = 7*250 \n",
    "\n",
    "    class_return = np.zeros(n_tests)\n",
    "    data_return = np.zeros((n_tests, n_channels, window_Length))\n",
    "\n",
    "    NO_valid_trial = 0\n",
    "    if training:\n",
    "        a = sio.loadmat(data_path+'A0'+str(subject)+'T.mat')\n",
    "    else:\n",
    "        a = sio.loadmat(data_path+'A0'+str(subject)+'E.mat')\n",
    "    a_data = a['data']\n",
    "    for ii in range(0,a_data.size):\n",
    "        a_data1 = a_data[0,ii]\n",
    "        a_data2= [a_data1[0,0]]\n",
    "        a_data3= a_data2[0]\n",
    "        a_X \t\t= a_data3[0]\n",
    "        a_trial \t= a_data3[1]\n",
    "        a_y \t\t= a_data3[2]\n",
    "        a_artifacts = a_data3[5]\n",
    "        for trial in range(0, a_trial.size):\n",
    "            if(a_artifacts[trial] != 0 and not all_trials):\n",
    "                continue\n",
    "            data_return[NO_valid_trial,:,:] = np.transpose(a_X[int(a_trial[trial]):(int(a_trial[trial])+window_Length),:22])\n",
    "            class_return[NO_valid_trial] = int(a_y[trial])\n",
    "            NO_valid_trial +=1\n",
    "\n",
    "        return data_return[0:NO_valid_trial,:,:], class_return[0:NO_valid_trial]\n",
    "   \n",
    "    \n",
    "\n",
    "#%%\n",
    "def standardize_data(X_train, X_test, channels): \n",
    "    # X_train & X_test :[Trials, MI-tasks, Channels, Time points]\n",
    "    for j in range(channels):\n",
    "          scaler = StandardScaler()\n",
    "          scaler.fit(X_train[:, 0, j, :])\n",
    "          X_train[:, 0, j, :] = scaler.transform(X_train[:, 0, j, :])\n",
    "          X_test[:, 0, j, :] = scaler.transform(X_test[:, 0, j, :])\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "#%%\n",
    "def get_data(path, subject, LOSO = False, isStandard = True):\n",
    "    # Define dataset parameters\n",
    "    \n",
    "    fs = 250          # sampling rate\n",
    "    t1 = int(1.5*fs)  # start time_point\n",
    "    t2 = int(6*fs)    # end time_point\n",
    "    T = t2-t1         # length of the MI trial (samples or time_points)\n",
    "    \n",
    "    # Load and split the dataset into training and testing \n",
    "    if LOSO:\n",
    "        # Loading and Dividing of the data set based on the \n",
    "        # 'Leave One Subject Out' (LOSO) evaluation approach. \n",
    "        X_train, y_train, X_test, y_test = load_data_LOSO(path, subject)\n",
    "    else:\n",
    "        # Loading and Dividing of the data set based on the subject-specific \n",
    "        # (subject-dependent) approach.In this approach, we used the same \n",
    "        # training and testing data as the original competition, i.e., trials \n",
    "        # in session 1 for training, and trials in session 2 for testing.  \n",
    "        path = path + 's{:}/'.format(subject+1)\n",
    "        X_train, y_train = load_data(path, subject+1, True)\n",
    "        X_test, y_test = load_data(path, subject+1, False)\n",
    "\n",
    "    # Prepare training data \t\n",
    "    N_tr, N_ch, _ = X_train.shape \n",
    "    X_train = X_train[:, :, t1:t2].reshape(N_tr, 1, N_ch, T)\n",
    "    y_train_onehot = (y_train-1).astype(int)\n",
    "    y_train_onehot = to_categorical(y_train_onehot)\n",
    "    # Prepare testing data \n",
    "    N_test, N_ch, _ = X_test.shape \n",
    "    X_test = X_test[:, :, t1:t2].reshape(N_test, 1, N_ch, T)\n",
    "    y_test_onehot = (y_test-1).astype(int)\n",
    "    y_test_onehot = to_categorical(y_test_onehot)\t\n",
    "    \n",
    "    # Standardize the data\n",
    "    if (isStandard == True):\n",
    "        X_train, X_test = standardize_data(X_train, X_test, N_ch)\n",
    "\n",
    "    return X_train, y_train, y_train_onehot, X_test, y_test, y_test_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ATCNet(n_classes, in_chans = 22, in_samples = 1125, n_windows = 3, attention = None, \n",
    "           eegn_F1 = 16, eegn_D = 2, eegn_kernelSize = 64, eegn_poolSize = 8, eegn_dropout=0.3, \n",
    "           tcn_depth = 2, tcn_kernelSize = 4, tcn_filters = 32, tcn_dropout = 0.3, \n",
    "           tcn_activation = 'elu', fuse = 'average'):\n",
    "    \"\"\" ATCNet model from Altaheri et al 2022.\n",
    "        See details at https://ieeexplore.ieee.org/abstract/document/9852687\n",
    "    \n",
    "        Notes\n",
    "        -----\n",
    "        The initial values in this model are based on the values identified by\n",
    "        the authors\n",
    "        \n",
    "        References\n",
    "        ----------\n",
    "        .. H. Altaheri, G. Muhammad and M. Alsulaiman, \"Physics-informed \n",
    "           attention temporal convolutional network for EEG-based motor imagery \n",
    "           classification,\" in IEEE Transactions on Industrial Informatics, 2022, \n",
    "           doi: 10.1109/TII.2022.3197419.\n",
    "    \"\"\"\n",
    "    input_1 = Input(shape = (1,in_chans, in_samples))   #     TensorShape([None, 1, 22, 1125])\n",
    "    input_2 = Permute((3,2,1))(input_1) \n",
    "    regRate=.25\n",
    "    numFilters = eegn_F1\n",
    "    F2 = numFilters*eegn_D\n",
    "\n",
    "    block1 = Conv_block(input_layer = input_2, F1 = eegn_F1, D = eegn_D, \n",
    "                        kernLength = eegn_kernelSize, poolSize = eegn_poolSize,\n",
    "                        in_chans = in_chans, dropout = eegn_dropout)\n",
    "    block1 = Lambda(lambda x: x[:,:,-1,:])(block1)\n",
    "     \n",
    "    # Sliding window \n",
    "    sw_concat = []   # to store concatenated or averaged sliding window outputs\n",
    "    for i in range(n_windows):\n",
    "        st = i\n",
    "        end = block1.shape[1]-n_windows+i+1\n",
    "        block2 = block1[:, st:end, :]\n",
    "        \n",
    "        # Attention_model\n",
    "        if attention is not None:\n",
    "            block2 = attention_block(block2, attention)\n",
    "\n",
    "        # Temporal convolutional network (TCN)\n",
    "        block3 = TCN_block(input_layer = block2, input_dimension = F2, depth = tcn_depth,\n",
    "                            kernel_size = tcn_kernelSize, filters = tcn_filters, \n",
    "                            dropout = tcn_dropout, activation = tcn_activation)\n",
    "        # Get feature maps of the last sequence\n",
    "        block3 = Lambda(lambda x: x[:,-1,:])(block3)\n",
    "        \n",
    "        # Outputs of sliding window: Average_after_dense or concatenate_then_dense\n",
    "        if(fuse == 'average'):\n",
    "            sw_concat.append(Dense(n_classes, kernel_constraint = max_norm(regRate))(block3))\n",
    "        elif(fuse == 'concat'):\n",
    "            if i == 0:\n",
    "                sw_concat = block3\n",
    "            else:\n",
    "                sw_concat = Concatenate()([sw_concat, block3])\n",
    "                \n",
    "    if(fuse == 'average'):\n",
    "        if len(sw_concat) > 1: # more than one window\n",
    "            sw_concat = tf.keras.layers.Average()(sw_concat[:])\n",
    "        else: # one window (# windows = 1)\n",
    "            sw_concat = sw_concat[0]\n",
    "    elif(fuse == 'concat'):\n",
    "        sw_concat = Dense(n_classes, kernel_constraint = max_norm(regRate))(sw_concat)\n",
    "            \n",
    "    \n",
    "    softmax = Activation('softmax', name = 'softmax')(sw_concat)\n",
    "    \n",
    "    return Model(inputs = input_1, outputs = softmax)\n",
    "\n",
    "#%% Convolutional (CV) block used in the ATCNet model\n",
    "def Conv_block(input_layer, F1=4, kernLength=64, poolSize=8, D=2, in_chans=22, dropout=0.1):\n",
    "    \"\"\" Conv_block\n",
    "    \n",
    "        Notes\n",
    "        -----\n",
    "        This block is the same as EEGNet with SeparableConv2D replaced by Conv2D \n",
    "        The original code for this model is available at: https://github.com/vlawhern/arl-eegmodels\n",
    "        See details at https://arxiv.org/abs/1611.08024\n",
    "    \"\"\"\n",
    "    F2= F1*D\n",
    "    block1 = Conv2D(F1, (kernLength, 1), padding = 'same',data_format='channels_last',use_bias = False)(input_layer)\n",
    "    block1 = BatchNormalization(axis = -1)(block1)\n",
    "    block2 = DepthwiseConv2D((1, in_chans), use_bias = False, \n",
    "                                    depth_multiplier = D,\n",
    "                                    data_format='channels_last',\n",
    "                                    depthwise_constraint = max_norm(1.))(block1)\n",
    "    block2 = BatchNormalization(axis = -1)(block2)\n",
    "    block2 = Activation('elu')(block2)\n",
    "    block2 = AveragePooling2D((8,1),data_format='channels_last')(block2)\n",
    "    block2 = Dropout(dropout)(block2)\n",
    "    block3 = Conv2D(F2, (16, 1),\n",
    "                            data_format='channels_last',\n",
    "                            use_bias = False, padding = 'same')(block2)\n",
    "    block3 = BatchNormalization(axis = -1)(block3)\n",
    "    block3 = Activation('elu')(block3)\n",
    "    \n",
    "    block3 = AveragePooling2D((poolSize,1),data_format='channels_last')(block3)\n",
    "    block3 = Dropout(dropout)(block3)\n",
    "    return block3\n",
    "\n",
    "#%% Temporal convolutional (TC) block used in the ATCNet model\n",
    "def TCN_block(input_layer,input_dimension,depth,kernel_size,filters,dropout,activation='relu'):\n",
    "    \"\"\" TCN_block from Bai et al 2018\n",
    "        Temporal Convolutional Network (TCN)\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        THe original code available at https://github.com/locuslab/TCN/blob/master/TCN/tcn.py\n",
    "        This implementation has a slight modification from the original code\n",
    "        and it is taken from the code by Ingolfsson et al at https://github.com/iis-eth-zurich/eeg-tcnet\n",
    "        See details at https://arxiv.org/abs/2006.00622\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        .. Bai, S., Kolter, J. Z., & Koltun, V. (2018).\n",
    "           An empirical evaluation of generic convolutional and recurrent networks\n",
    "           for sequence modeling.\n",
    "           arXiv preprint arXiv:1803.01271.\n",
    "    \"\"\"    \n",
    "    \n",
    "    block = Conv1D(filters,kernel_size=kernel_size,dilation_rate=1,activation='linear',\n",
    "                   padding = 'causal',kernel_initializer='he_uniform')(input_layer)\n",
    "    block = BatchNormalization()(block)\n",
    "    block = Activation(activation)(block)\n",
    "    block = Dropout(dropout)(block)\n",
    "    block = Conv1D(filters,kernel_size=kernel_size,dilation_rate=1,activation='linear',\n",
    "                   padding = 'causal',kernel_initializer='he_uniform')(block)\n",
    "    block = BatchNormalization()(block)\n",
    "    block = Activation(activation)(block)\n",
    "    block = Dropout(dropout)(block)\n",
    "    if(input_dimension != filters):\n",
    "        conv = Conv1D(filters,kernel_size=1,padding='same')(input_layer)\n",
    "        added = Add()([block,conv])\n",
    "    else:\n",
    "        added = Add()([block,input_layer])\n",
    "    out = Activation(activation)(added)\n",
    "    \n",
    "    for i in range(depth-1):\n",
    "        block = Conv1D(filters,kernel_size=kernel_size,dilation_rate=2**(i+1),activation='linear',\n",
    "                   padding = 'causal',kernel_initializer='he_uniform')(out)\n",
    "        block = BatchNormalization()(block)\n",
    "        block = Activation(activation)(block)\n",
    "        block = Dropout(dropout)(block)\n",
    "        block = Conv1D(filters,kernel_size=kernel_size,dilation_rate=2**(i+1),activation='linear',\n",
    "                   padding = 'causal',kernel_initializer='he_uniform')(block)\n",
    "        block = BatchNormalization()(block)\n",
    "        block = Activation(activation)(block)\n",
    "        block = Dropout(dropout)(block)\n",
    "        added = Add()([block, out])\n",
    "        out = Activation(activation)(added)\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_learning_curves(history):\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def draw_confusion_matrix(cf_matrix, sub, results_path):\n",
    "    # Generate confusion matrix plot\n",
    "    display_labels = ['Left hand', 'Right hand','Foot','Tongue']\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cf_matrix, \n",
    "                                display_labels=display_labels)\n",
    "    disp.plot()\n",
    "    disp.ax_.set_xticklabels(display_labels, rotation=12)\n",
    "    plt.title('Confusion Matrix of Subject: ' + sub )\n",
    "    plt.savefig(results_path + '/subject_' + sub + '.png')\n",
    "    plt.show()\n",
    "\n",
    "def draw_performance_barChart(num_sub, metric, label):\n",
    "    fig, ax = plt.subplots()\n",
    "    x = list(range(1, num_sub+1))\n",
    "    ax.bar(x, metric, 0.5, label=label)\n",
    "    ax.set_ylabel(label)\n",
    "    ax.set_xlabel(\"Subject\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_title('Model '+ label + ' per subject')\n",
    "    ax.set_ylim([0,1])\n",
    "    \n",
    "    \n",
    "#%% Training \n",
    "def train(dataset_conf, train_conf, results_path):\n",
    "    # Get the current 'IN' time to calculate the overall training time\n",
    "    in_exp = time.time()\n",
    "    # Create a file to store the path of the best model among several runs\n",
    "    best_models = open(results_path + \"/best models.txt\", \"w\")\n",
    "    # Create a file to store performance during training\n",
    "    log_write = open(results_path + \"/log.txt\", \"w\")\n",
    "    # Create a .npz file (zipped archive) to store the accuracy and kappa metrics \n",
    "    # for all runs (to calculate average accuracy/kappa over all runs)\n",
    "    perf_allRuns = open(results_path + \"/perf_allRuns.npz\", 'wb')\n",
    "    \n",
    "    # Get dataset paramters\n",
    "    n_sub = dataset_conf.get('n_sub')\n",
    "    data_path = dataset_conf.get('data_path')\n",
    "    isStandard = dataset_conf.get('isStandard')\n",
    "    LOSO = dataset_conf.get('LOSO')\n",
    "    # Get training hyperparamters\n",
    "    batch_size = train_conf.get('batch_size')\n",
    "    epochs = train_conf.get('epochs')\n",
    "    patience = train_conf.get('patience')\n",
    "    lr = train_conf.get('lr')\n",
    "    LearnCurves = train_conf.get('LearnCurves') # Plot Learning Curves?\n",
    "    n_train = train_conf.get('n_train')\n",
    "    model_name = train_conf.get('model')\n",
    "\n",
    "    # Initialize variables\n",
    "    acc = np.zeros((n_sub, n_train))\n",
    "    kappa = np.zeros((n_sub, n_train))\n",
    "    \n",
    "    # Iteration over subjects \n",
    "    for sub in range(n_sub): # (num_sub): for all subjects, (i-1,i): for the ith subject.\n",
    "        # Get the current 'IN' time to calculate the subject training time\n",
    "        in_sub = time.time()\n",
    "        print('\\nTraining on subject ', sub+1)\n",
    "        log_write.write( '\\nTraining on subject '+ str(sub+1) +'\\n')\n",
    "        # Initiating variables to save the best subject accuracy among multiple runs.\n",
    "        BestSubjAcc = 0 \n",
    "        bestTrainingHistory = [] \n",
    "        # Get training and test data\n",
    "        X_train, _, y_train_onehot, X_test, _, y_test_onehot = get_data(\n",
    "            data_path, sub, LOSO, isStandard)\n",
    "        \n",
    "        # Iteration over multiple runs \n",
    "        for train in range(n_train): # How many repetitions of training for subject i.\n",
    "            # Get the current 'IN' time to calculate the 'run' training time\n",
    "            in_run = time.time()\n",
    "            # Create folders and files to save trained models for all runs\n",
    "            filepath = results_path + '/saved models/run-{}'.format(train+1)\n",
    "            if not os.path.exists(filepath):\n",
    "                os.makedirs(filepath)        \n",
    "            filepath = filepath + '/subject-{}.h5'.format(sub+1)\n",
    "            \n",
    "            # Create the model\n",
    "            model = getModel(model_name)\n",
    "            # Compile and train the model\n",
    "            model.compile(loss=categorical_crossentropy, optimizer=Adam(learning_rate=lr), metrics=['accuracy'])          \n",
    "            callbacks = [\n",
    "                ModelCheckpoint(filepath, monitor='val_accuracy', verbose=0, \n",
    "                                save_best_only=True, save_weights_only=True, mode='max'),\n",
    "                EarlyStopping(monitor='val_accuracy', verbose=1, mode='max', patience=patience)\n",
    "            ]\n",
    "            history = model.fit(X_train, y_train_onehot, validation_data=(X_test, y_test_onehot), \n",
    "                                epochs=epochs, batch_size=batch_size, callbacks=callbacks, verbose=0)\n",
    "\n",
    "            # Evaluate the performance of the trained model. \n",
    "            # Here we load the Trained weights from the file saved in the hard \n",
    "            # disk, which should be the same as the weights of the current model.\n",
    "            model.load_weights(filepath)\n",
    "            y_pred = model.predict(X_test).argmax(axis=-1)\n",
    "            labels = y_test_onehot.argmax(axis=-1)\n",
    "            acc[sub, train]  = accuracy_score(labels, y_pred)\n",
    "            kappa[sub, train] = cohen_kappa_score(labels, y_pred)\n",
    "              \n",
    "            # Get the current 'OUT' time to calculate the 'run' training time\n",
    "            out_run = time.time()\n",
    "            # Print & write performance measures for each run\n",
    "            info = 'Subject: {}   Train no. {}   Time: {:.1f} m   '.format(sub+1, train+1, ((out_run-in_run)/60))\n",
    "            info = info + 'Test_acc: {:.4f}   Test_kappa: {:.4f}'.format(acc[sub, train], kappa[sub, train])\n",
    "            print(info)\n",
    "            log_write.write(info +'\\n')\n",
    "            # If current training run is better than previous runs, save the history.\n",
    "            if(BestSubjAcc < acc[sub, train]):\n",
    "                 BestSubjAcc = acc[sub, train]\n",
    "                 bestTrainingHistory = history\n",
    "        \n",
    "        # Store the path of the best model among several runs\n",
    "        best_run = np.argmax(acc[sub,:])\n",
    "        filepath = '/saved models/run-{}/subject-{}.h5'.format(best_run+1, sub+1)+'\\n'\n",
    "        best_models.write(filepath)\n",
    "        # Get the current 'OUT' time to calculate the subject training time\n",
    "        out_sub = time.time()\n",
    "        # Print & write the best subject performance among multiple runs\n",
    "        info = '----------\\n'\n",
    "        info = info + 'Subject: {}   best_run: {}   Time: {:.1f} m   '.format(sub+1, best_run+1, ((out_sub-in_sub)/60))\n",
    "        info = info + 'acc: {:.4f}   avg_acc: {:.4f} +- {:.4f}   '.format(acc[sub, best_run], np.average(acc[sub, :]), acc[sub,:].std() )\n",
    "        info = info + 'kappa: {:.4f}   avg_kappa: {:.4f} +- {:.4f}'.format(kappa[sub, best_run], np.average(kappa[sub, :]), kappa[sub,:].std())\n",
    "        info = info + '\\n----------'\n",
    "        print(info)\n",
    "        log_write.write(info+'\\n')\n",
    "        # Plot Learning curves \n",
    "        if (LearnCurves == True):\n",
    "            print('Plot Learning Curves ....... ')\n",
    "            draw_learning_curves(bestTrainingHistory)\n",
    "          \n",
    "    # Get the current 'OUT' time to calculate the overall training time\n",
    "    out_exp = time.time()\n",
    "    info = '\\nTime: {:.1f} h   '.format( (out_exp-in_exp)/(60*60) )\n",
    "    print(info)\n",
    "    log_write.write(info+'\\n')\n",
    "    \n",
    "    # Store the accuracy and kappa metrics as arrays for all runs into a .npz \n",
    "    # file format, which is an uncompressed zipped archive, to calculate average\n",
    "    # accuracy/kappa over all runs.\n",
    "    np.savez(perf_allRuns, acc = acc, kappa = kappa)\n",
    "    \n",
    "    # Close open files \n",
    "    best_models.close()   \n",
    "    log_write.close() \n",
    "    perf_allRuns.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel(model_name):\n",
    "    # Select the model\n",
    "    if(model_name == 'ATCNet'):\n",
    "        # Train using the proposed model (ATCNet): https://doi.org/10.1109/TII.2022.3197419\n",
    "        model = ATCNet( \n",
    "            # Dataset parameters\n",
    "            n_classes = 4, \n",
    "            in_chans = 22, \n",
    "            in_samples = 1125, \n",
    "            # Sliding window (SW) parameter\n",
    "            n_windows = 5, \n",
    "            # Attention (AT) block parameter\n",
    "            attention = 'mha', # Options: None, 'mha','mhla', 'cbam', 'se'\n",
    "            # Convolutional (CV) block parameters\n",
    "            eegn_F1 = 16,\n",
    "            eegn_D = 2, \n",
    "            eegn_kernelSize = 64,\n",
    "            eegn_poolSize = 7,\n",
    "            eegn_dropout = 0.3,\n",
    "            # Temporal convolutional (TC) block parameters\n",
    "            tcn_depth = 2, \n",
    "            tcn_kernelSize = 4,\n",
    "            tcn_filters = 32,\n",
    "            tcn_dropout = 0.3, \n",
    "            tcn_activation='elu'\n",
    "            )     \n",
    "    elif(model_name == 'TCNet_Fusion'):\n",
    "        # Train using TCNet_Fusion: https://doi.org/10.1016/j.bspc.2021.102826\n",
    "        model = models.TCNet_Fusion(n_classes = 4)      \n",
    "    elif(model_name == 'EEGTCNet'):\n",
    "        # Train using EEGTCNet: https://arxiv.org/abs/2006.00622\n",
    "        model = models.EEGTCNet(n_classes = 4)          \n",
    "    elif(model_name == 'EEGNet'):\n",
    "        # Train using EEGNet: https://arxiv.org/abs/1611.08024\n",
    "        model = models.EEGNet_classifier(n_classes = 4) \n",
    "    elif(model_name == 'EEGNeX'):\n",
    "        # Train using EEGNeX: https://arxiv.org/abs/2207.12369\n",
    "        model = models.EEGNeX_8_32(n_timesteps = 1125 , n_features = 22, n_outputs = 4)\n",
    "    elif(model_name == 'DeepConvNet'):\n",
    "        # Train using DeepConvNet: https://doi.org/10.1002/hbm.23730\n",
    "        model = models.DeepConvNet(nb_classes = 4 , Chans = 22, Samples = 1125)\n",
    "    elif(model_name == 'ShallowConvNet'):\n",
    "        # Train using ShallowConvNet: https://doi.org/10.1002/hbm.23730\n",
    "        model = models.ShallowConvNet(nb_classes = 4 , Chans = 22, Samples = 1125)\n",
    "    else:\n",
    "        raise Exception(\"'{}' model is not supported yet!\".format(model_name))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on subject  1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-fe64b6d2ee91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_conf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_conf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Evaluate the model based on the weights saved in the '/results' folder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-bc2376485b00>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataset_conf, train_conf, results_path)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;31m# Get training and test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         X_train, _, y_train_onehot, X_test, _, y_test_onehot = get_data(\n\u001b[1;32m---> 81\u001b[1;33m             data_path, sub, LOSO, isStandard)\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;31m# Iteration over multiple runs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-97ffeab84c76>\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(path, subject, LOSO, isStandard)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mt2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_ch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[0my_train_onehot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m     \u001b[0my_train_onehot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_onehot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m     \u001b[1;31m# Prepare testing data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[0mN_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_ch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\dlenv\\lib\\site-packages\\keras\\utils\\np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes, dtype)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m     \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[0mcategorical\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mamax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\dlenv\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2753\u001b[0m     \"\"\"\n\u001b[0;32m   2754\u001b[0m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[1;32m-> 2755\u001b[1;33m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0m\u001b[0;32m   2756\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2757\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\dlenv\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "# Get dataset path\n",
    "data_path = 'C:/Users/ayu08/OneDrive/Documents/MProject-1/BCI-Competition-vi/BCI Competition IV-2a(mat files)/'\n",
    "\n",
    "# Create a folder to store the results of the experiment\n",
    "results_path = os.getcwd() + \"/results\"\n",
    "if not  os.path.exists(results_path):\n",
    "  os.makedirs(results_path)   # Create a new directory if it does not exist \n",
    "\n",
    "# Set dataset paramters \n",
    "dataset_conf = { 'n_classes': 4, 'n_sub': 9, 'n_channels': 22, 'data_path': data_path,\n",
    "            'isStandard': False, 'LOSO': False}\n",
    "# Set training hyperparamters\n",
    "train_conf = { 'batch_size': 64, 'epochs': 1000, 'patience': 300, 'lr': 0.0009,\n",
    "              'LearnCurves': True, 'n_train': 10, 'model':'ATCNet'}\n",
    "\n",
    "# Train the model\n",
    "train(dataset_conf, train_conf, results_path)\n",
    "\n",
    "# Evaluate the model based on the weights saved in the '/results' folder\n",
    "#model = getModel(train_conf.get('model'))\n",
    "#test(model, dataset_conf, results_path)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataset_conf, results_path, allRuns = True):\n",
    "    # Open the  \"Log\" file to write the evaluation results \n",
    "    log_write = open(results_path + \"/log.txt\", \"a\")\n",
    "    # Open the file that stores the path of the best models among several random runs.\n",
    "    best_models = open(results_path + \"/best models.txt\", \"r\")   \n",
    "    \n",
    "    # Get dataset paramters\n",
    "    n_classes = dataset_conf.get('n_classes')\n",
    "    n_sub = dataset_conf.get('n_sub')\n",
    "    data_path = dataset_conf.get('data_path')\n",
    "    isStandard = dataset_conf.get('isStandard')\n",
    "    LOSO = dataset_conf.get('LOSO')\n",
    "    \n",
    "    # Initialize variables\n",
    "    acc_bestRun = np.zeros(n_sub)\n",
    "    kappa_bestRun = np.zeros(n_sub)  \n",
    "    cf_matrix = np.zeros([n_sub, n_classes, n_classes])\n",
    "\n",
    "    # Calculate the average performance (average accuracy and K-score) for \n",
    "    # all runs (experiments) for each subject.\n",
    "    if(allRuns): \n",
    "        # Load the test accuracy and kappa metrics as arrays for all runs from a .npz \n",
    "        # file format, which is an uncompressed zipped archive, to calculate average\n",
    "        # accuracy/kappa over all runs.\n",
    "        perf_allRuns = open(results_path + \"/perf_allRuns.npz\", 'rb')\n",
    "        perf_arrays = np.load(perf_allRuns)\n",
    "        acc_allRuns = perf_arrays['acc']\n",
    "        kappa_allRuns = perf_arrays['kappa']\n",
    "    \n",
    "    # Iteration over subjects \n",
    "    for sub in range(n_sub): # (num_sub): for all subjects, (i-1,i): for the ith subject.\n",
    "        # Load data\n",
    "        _, _, _, X_test, _, y_test_onehot = get_data(data_path, sub, LOSO, isStandard)\n",
    "        # Load the best model out of multiple random runs (experiments).\n",
    "        filepath = best_models.readline()\n",
    "        model.load_weights(results_path + filepath[:-1])\n",
    "        # Predict MI task\n",
    "        y_pred = model.predict(X_test).argmax(axis=-1)\n",
    "        # Calculate accuracy and K-score\n",
    "        labels = y_test_onehot.argmax(axis=-1)\n",
    "        acc_bestRun[sub] = accuracy_score(labels, y_pred)\n",
    "        kappa_bestRun[sub] = cohen_kappa_score(labels, y_pred)\n",
    "        # Calculate and draw confusion matrix\n",
    "        cf_matrix[sub, :, :] = confusion_matrix(labels, y_pred, normalize='pred')\n",
    "        draw_confusion_matrix(cf_matrix[sub, :, :], str(sub+1), results_path)\n",
    "        \n",
    "        # Print & write performance measures for each subject\n",
    "        info = 'Subject: {}   best_run: {:2}  '.format(sub+1, (filepath[filepath.find('run-')+4:filepath.find('/sub')]) )\n",
    "        info = info + 'acc: {:.4f}   kappa: {:.4f}   '.format(acc_bestRun[sub], kappa_bestRun[sub] )\n",
    "        if(allRuns): \n",
    "            info = info + 'avg_acc: {:.4f} +- {:.4f}   avg_kappa: {:.4f} +- {:.4f}'.format(\n",
    "                np.average(acc_allRuns[sub, :]), acc_allRuns[sub,:].std(),\n",
    "                np.average(kappa_allRuns[sub, :]), kappa_allRuns[sub,:].std() )\n",
    "        print(info)\n",
    "        log_write.write('\\n'+info)\n",
    "      \n",
    "    # Print & write the average performance measures for all subjects     \n",
    "    info = '\\nAverage of {} subjects - best runs:\\nAccuracy = {:.4f}   Kappa = {:.4f}\\n'.format(\n",
    "        n_sub, np.average(acc_bestRun), np.average(kappa_bestRun)) \n",
    "    if(allRuns): \n",
    "        info = info + '\\nAverage of {} subjects x {} runs (average of {} experiments):\\nAccuracy = {:.4f}   Kappa = {:.4f}'.format(\n",
    "            n_sub, acc_allRuns.shape[1], (n_sub * acc_allRuns.shape[1]),\n",
    "            np.average(acc_allRuns), np.average(kappa_allRuns)) \n",
    "    print(info)\n",
    "    log_write.write(info)\n",
    "    \n",
    "    # Draw a performance bar chart for all subjects \n",
    "    draw_performance_barChart(n_sub, acc_bestRun, 'Accuracy')\n",
    "    draw_performance_barChart(n_sub, kappa_bestRun, 'K-score')\n",
    "    # Draw confusion matrix for all subjects (average)\n",
    "    draw_confusion_matrix(cf_matrix.mean(0), 'All', results_path)\n",
    "    # Close open files     \n",
    "    log_write.close() \n",
    "    \n",
    "    \n",
    "#%%\n",
    "\n",
    "    \n",
    "    \n",
    "#%%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
